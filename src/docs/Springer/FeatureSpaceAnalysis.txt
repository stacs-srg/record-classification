In machine learning tasks the measurable attributes that define the data are called features. In the case of text classification tasks we are interested in the words that make up the texts in questions, hence, these are our features.

The size of the feature space, ie the number of different words that occur in the corpus of documents used for training and in classification, is an important property to consider. Very large feature spaces can be problematic as they are harder for the machine learning algorithms to learn effectivey. 

We analyse the feature space using "feature frequency - inverse code frequency" statistics.

These are analogous to "tf-idf" or "term frequency - inverse document frequency" statistics.

Instead of thinking of terms and documents we think of features and codes.

Ff-icf is an information measure intended to reflect how important a feature is for predicting a code.

It is calculated from a collection of records for which we know "gold standard" codes.

A high ff-icf value indicates that the feature occurs often in conjunction with the code and infrequently in conjunction with other codes.

A low ff-icf score indicates that the feature does not occur often in conjunction with the code or appears very frequently with other codes.

For a collection of records we may calculate "ff-icf" values for every feature-code pair.

We do this for our "training" and "validation" sets and construct a graph for each code.

The graph makes a pair wise comparison of the "ff-icf" statistics for each data set for the code in question.

We at most plot the 15 highest scoring features (from data set 1's point of view - we bias toward one data set -> perhaps training set most sensible).

This allows us to say e.g. feature "i" is important for predicting code "A" in data set 1 but is less important
for predicting code "A" in data set 2.

These graphs give us a nice way of seeing which features are important for predicting which codes.

They also allow us to analyse our system's performance and account for error.

If our system gives poor performance for a particular code at validation then we may consult the graph to find out why.

One may expect that poor performance for particular codes may be the result of poor "feature overlap/coverage".

For example, features with high ff-icf scores for the validation set may have very low ff-icf scores for the
same code in the training set -> this means that our models have not been trained to recognise these words as being predictors of the code.
