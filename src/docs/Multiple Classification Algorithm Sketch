Multiple classification algorithm sketch.


1. Tokenise original description into n tokens  ->  {T1, T2, T3, … ,Tn}

2. Enumerate all possible token n-grams (subsets formed by consecutive tokens)
Let’s call our set of n-grams G.

3. Classify each subset in G using a machine learning algorithm.

4. Discard classifications for which we have a low confidence in
the classification being correct.

5. Enumerate all valid combinations of these n-gram/code pairs.
We have two conditions by which we define validity.
Both have to hold for a set of classifications to be deemed valid.
    - codes must be from distinct hierarchies
    i.e. the codes 952 and 95240 would not be allowed to appear in
    combination as one is a decendent of the other in the hierarchy
    - the union of the tokens from the n-grams must be a subset
    of the tokens from the original string
    i.e. a word cannot be used in the prediction of more than one
    code in a set of classifications

6. Calculate a 'goodness' score for each valid set of classifications
    - our current goodness measure is calculated like so:
    - for each classification in the set we multiply the confidence in
    the classification by the length of the n-gram which predicted the classification.
    - the sum of these values is the goodness of the set

7. We classify the original description to the set of classifications which has the
best goodness score
